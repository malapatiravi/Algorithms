{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": "import rosbag, sys, csv\nimport os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(79049, 9)\n(79049, 9)\n"
    }
   ],
   "source": "\n# get files\n__curr_wd__ = os.path.realpath(os.getcwd())\n__DOCG_file__ = os.path.realpath((os.getcwd())) + '/' + 'DOCGObjectTracks2.csv'\n__OF_file__ = os.path.realpath(os.getcwd()) + '/' + 'fusedTracks2.csv'\n\nwith open(__DOCG_file__, 'r') as docg_file:\n    docg_objects = list(csv.reader(docg_file, delimiter = ','))\n    \nwith open(__OF_file__, 'r') as of_file:\n    of_objects = list(csv.reader(of_file, delimiter = ','))\n\ndocg_objects = docg_objects[1:] #remove first row, that is header\ndocg_objects = np.array(docg_objects).astype(\"float\") # convert all features to float\n\nof_objects = of_objects[1:] # remove the first row that is header\nof_objects = np.array(of_objects).astype(\"float\") # convert all fatures to float\n\nprint(docg_objects.shape)\nprint(of_objects.shape)\n# print(of_objects[1,:])\n# print(of_objects[79048,:])\n"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1. 1. 1. ... 1. 1. 1.]\n(79049, 19)\n"
    }
   ],
   "source": "# create True class data\n# the data in of and docg objects have same ids so they are True class and matches\ntrue_class_data = np.concatenate([docg_objects, of_objects], axis =1 ) # concatenate the data\ncol_append = np.zeros(shape=(79049,1)) # create classification column\n\ntrue_class_data = np.append(true_class_data,col_append, 1)\n\nfor row in true_class_data: # for each row\n    if row[0] == row[9]: # if the ids match then make the classification as 1\n        row[18] = 1\nprint(true_class_data[:,18])   # test by printing the column, the 18th column has the classification\n\nprint(true_class_data.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0. 0. 0. ... 0. 0. 0.]\n(79049, 19)\n"
    }
   ],
   "source": "# create False class data\n# roll of_objects move first half OF data to last and last half of data to first\nof_objects_roll = np.roll(of_objects, 39524, axis = 0)\nfalse_class_data = np.concatenate([docg_objects, of_objects_roll], axis = 1) # concat the data\nclass_col_append = np.zeros(shape=(79049, 1)) # create classification column\nfalse_class_data = np.append(false_class_data, class_col_append, 1)\nprint(false_class_data[:,18])\nprint(false_class_data.shape)\n# x = np.arange(10)\n# x = np.reshape(x,(5,2))\n# print(x)\n# x = np.roll(x,2, axis = 0)\n# print(x)"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(94858, 16)\n(94858,)\n(63240, 16)\n(63240,)\n(2000,)\n"
    }
   ],
   "source": "#Prepare the data for ML\n#delete the skewed data\ntrue_class_Xy = true_class_data\nfalse_class_Xy = false_class_data\ntrue_class_Xy = np.delete(true_class_Xy, [0,9], 1)\nfalse_class_Xy = np.delete(false_class_Xy, [0,9], 1)\n\nXy = np.concatenate([true_class_Xy, false_class_Xy], axis = 0)\n\n#for i in range(20):\n#    Xy\n    \nnp.random.shuffle(Xy)\n# from sklearn.model_selection import train_test_split\nX = Xy[:,0:16]\ny = Xy[:,16]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\nX_train = X_train[0:2000,:]\ny_train = y_train[0:2000]\nprint(y_train.shape)\nX_test = X_test[2100:65000,:]\ny_test = y_test[2100:65000]\n# true_class_X = true_class_Xy[:]\n#delete the unwanted columns\n#arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n#print(arr)\n#arr = np.delete(arr, 1, 1)\n#print(arr)"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.987144259077527\n60354\n786\n"
    }
   ],
   "source": "# Neural Network MLP classifier\nfrom sklearn.neural_network import MLPClassifier\nNNclf = MLPClassifier(random_state = 0, hidden_layer_sizes=(20), activation = 'relu')\nNNclf.fit(X_train, y_train)\nprint(NNclf.score(X_test, y_test))\ny_pred = NNclf.predict(X_test)\ncount = 0\nNcount = 0\nfor i in range(len(y_pred)):\n    if y_pred[i] == y_test[i]:\n        count = count+1\n    else:\n        Ncount = Ncount+1\nprint(count)\nprint(Ncount)\n"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.9886706948640483\n[0. 0. 1. ... 1. 0. 0.]\n13090\n150\n"
    }
   ],
   "source": "# RandomForest classifier\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nclfr = rfc(n_estimators=120)\nclfr.fit(X_train, y_train)\nprint(clfr.score(X_test,y_test))\n#print(clfr.feature_importances_)\ny_pred = clfr.predict(X_test)\nprint(y_pred)\ncount = 0\nNcount = 0\nfor i in range(len(y_pred)):\n    if y_pred[i] == y_test[i]:\n        count = count+1\n    else:\n        Ncount = Ncount+1\nprint(count)\nprint(Ncount)\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.9668\n9668\n332\n"
    }
   ],
   "source": "## KNN Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nKNNclf = KNeighborsClassifier(n_neighbors=1)\nKNNclf.fit(X_train, y_train)\nprint(KNNclf.score(X_test, y_test))\ny_pred = KNNclf.predict(X_test)\ncount = 0\nNcount = 0\nfor i in range(len(y_pred)):\n    if y_pred[i] == y_test[i]:\n        count = count+1\n    else:\n        Ncount = Ncount+1\nprint(count)\nprint(Ncount)"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.9221\n[0. 0. 1. ... 1. 0. 0.]\n9221\n779\n"
    }
   ],
   "source": "#Lets test with SVM algorithm\nfrom sklearn import svm\nclf = svm.SVC(probability=True)\nclf.fit(X_train, y_train)\nprint(clf.score(X_test,y_test))\ny_pred = clf.predict(X_test)\nprint(y_pred)\ncount = 0\nNcount = 0\nfor i in range(len(y_pred)):\n    if y_pred[i] == y_test[i]:\n        count = count+1\n    else:\n        Ncount = Ncount+1\nprint(count)\nprint(Ncount)"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.8775\n1755\n245\n"
    }
   ],
   "source": "# DecisionTrees\nfrom sklearn import tree\nDTclf = tree.DecisionTreeClassifier()\nDTclf.fit(X_train, y_train)\nprint(DTclf.score(X_test, y_test))\ny_pred = DTclf.predict(X_test)\ncount = 0\nNcount = 0\nfor i in range(len(y_pred)):\n    if y_pred[i] == y_test[i]:\n        count = count+1\n    else:\n        Ncount = Ncount+1\nprint(count)\nprint(Ncount)"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.4975\n995\n1005\n"
    }
   ],
   "source": "#NAiveBayes Classifier\nfrom sklearn.naive_bayes import GaussianNB\nNBclf = GaussianNB()\nNBclf.fit(X_train, y_train)\nprint(NBclf.score(X_test, y_test))\ny_pred = NBclf.predict(X_test)\ncount = 0\nNcount = 0\nfor i in range(len(y_pred)):\n    if y_pred[i] == y_test[i]:\n        count = count+1\n    else:\n        Ncount = Ncount+1\nprint(count)\nprint(Ncount)\n"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/kjg9xn/.local/lib/python2.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=array, inputs=array)`\n  \n"
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-8cfa10ab6f9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msiamese_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/kjg9xn/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kjg9xn/anaconda2/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;31m# Check for redundancy in inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1531\u001b[0m             raise ValueError('The list of inputs passed to the model '\n\u001b[1;32m   1532\u001b[0m                              \u001b[0;34m'is redundant. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten, MaxPooling2D\nfrom keras.models import Model, Sequential\n\n\nsiamese_net = Model(input=X_train, output=y_train)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
